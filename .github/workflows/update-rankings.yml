name: Update Rankings

on:
  schedule:
    - cron: '0 0 * * *' # every day at midnight
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape-and-rank:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v3

      - name: Install Python Dependencies
        # BeautifulSoup is required for HTML parsing
        run: pip install beautifulsoup4

      - name: Load Configuration Variables
        id: load_config
        # Source the config file to load variables into the shell environment
        run: |
          if [[ -f "./config.env" ]]; then
            echo "Loading variables from config.env"
            source ./config.env
            # Set variables as GitHub environment variables for subsequent steps
            echo "TEAM_NAME=$TEAM_NAME" >> $GITHUB_ENV
            echo "LEAGUE_URL=$LEAGUE_URL" >> $GITHUB_ENV
            echo "FIXTURES_URL=$FIXTURES_URL" >> $GITHUB_ENV
            echo "SEASON=$SEASON" >> $GITHUB_ENV
            echo "LEAGUE_ID_NAME=$LEAGUE_ID_NAME" >> $GITHUB_ENV
          else
            echo "‚ùå ERROR: config.env file not found. Please create it."
            exit 1
          fi

      - name: Fetch and Parse Standings Table (Python Scraper)
        # Uses python3 which is pre-installed on the runner
        run: |
          mkdir -p data tmp
          
          # 1. Fetch the HTML content
          echo "Fetching HTML standings from $LEAGUE_URL"
          curl -s "$LEAGUE_URL" -o tmp/bbc-standings.html
          
          # 2. Python script to find the specific table and parse it into JSON
          python3 - <<EOF
import json
from bs4 import BeautifulSoup
import sys
import os

# Retrieve environment variables set in the previous step
LEAGUE_ID_NAME = os.environ.get('LEAGUE_ID_NAME', 'League')
SEASON = os.environ.get('SEASON', 'Unknown')

try:
    with open('tmp/bbc-standings.html', 'r', encoding='utf-8') as f:
        html_content = f.read()
except FileNotFoundError:
    print("Error: Could not read tmp/bbc-standings.html")
    sys.exit(1)

soup = BeautifulSoup(html_content, 'html.parser')

# --- CRITICAL: Find the main standings table ---
# BBC tables often have classes containing 'table' and a generated ssrcss unique identifier.
standings_table = soup.find('table', class_=lambda c: c and ('table' in c.lower() and 'ssrcss' in c))

if not standings_table:
    print("Error: Could not find the main standings table (check BBC class names).")
    # Log the fetched content for manual inspection if scraping fails
    with open('tmp/debug_html_start.txt', 'w') as dbg:
        dbg.write("HTML CONTENT START\\n" + html_content[:4000]) 
    print("A debug file 'tmp/debug_html_start.txt' was created to inspect the fetched content. Check your logs.")
    sys.exit(1)

data_list = []
tbody = standings_table.find('tbody')

if not tbody:
    print("Error: Table body not found.")
    sys.exit(1)
    
for row in tbody.find_all('tr'):
    # Find all table data cells (th for rank, td for stats)
    cols = row.find_all(['th', 'td'])
    
    # Ensure it's a full data row (expected 10+ columns)
    if len(cols) < 10:
        continue 

    try:
        # Extract Rank from the first cell (cols[0])
        rank_text = cols[0].get_text(strip=True)
        # Extract Team Name from the second cell (cols[1]) - handles nested spans
        team_name_element = cols[1].find('span', class_='ssrcss-1c9v1s-Name')
        team_name = team_name_element.get_text(strip=True) if team_name_element else cols[1].get_text(strip=True)
        
        # --- Map data structure ---
        team_data = {
            "rank": int(rank_text),
            "team_name": team_name,
            "played": int(cols[2].get_text(strip=True)),
            "won": int(cols[3].get_text(strip=True)),
            "drawn": int(cols[4].get_text(strip=True)),
            "lost": int(cols[5].get_text(strip=True)),
            "goals_for": int(cols[6].get_text(strip=True)),
            "goals_against": int(cols[7].get_text(strip=True)),
            "goal_difference": int(cols[8].get_text(strip=True)),
            "points": int(cols[9].get_text(strip=True)),
        }
        data_list.append(team_data)
        
    except Exception as e:
        print(f"Skipping row due to parsing error: {e}")
        # Note: If this happens often, the column indexing is wrong.
        continue

# Create final JSON output structure
final_json = {
    "league_info": {
        "name": LEAGUE_ID_NAME,
        "season": SEASON
    },
    "standings": data_list
}

with open('data/standings.json', 'w') as outfile:
    json.dump(final_json, outfile, indent=4)
    
print("‚úÖ Standings JSON generated successfully: data/standings.json")
EOF

      - name: Fetch Upcoming Fixtures (HTML)
        run: |
          # Fetch fixtures page HTML for future processing (e.g., ICS file creation)
          mkdir -p public/cal
          echo "Fetching fixtures from $FIXTURES_URL"
          curl -s "$FIXTURES_URL" -o public/cal/fixtures.html
          echo "Future ICS calendar source saved as public/cal/fixtures.html"

      - name: Generate League Info File
        run: |
          # This is the dependency for the second workflow (update_standings)
          echo "{\"league_id\": \"$LEAGUE_ID_NAME\", \"season\": \"$SEASON\"}" > data/league-info.json

      - name: Commit and Push
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@users.noreply.github.com"
          git add data/league-info.json data/standings.json public/cal/fixtures.html
          if git diff --cached --quiet; then
            echo "üü¢ No changes to commit."
          else
            git commit -m "Scrape update: Standings and fixtures from BBC Sport for $LEAGUE_ID_NAME"
            git push
          fi
