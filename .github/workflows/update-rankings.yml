name: Update Football Data from BBC Sport

on:
  schedule:
    # Runs every day at 02:00 UTC to ensure league tables are settled.
    - cron: '0 2 * * *'
  workflow_dispatch: # Allows manual running

permissions:
  contents: write

jobs:
  scrape-and-process:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Python Dependencies
        run: pip install beautifulsoup4 requests

      - name: Load Configuration Variables and Build URLs
        id: load_config
        run: |
          if [[ -f "./config.env" ]]; then
            echo "✅ Loading variables from config.env"
            source ./config.env

            # --- Build full URLs from slugs ---
            LEAGUE_URL="https://www.bbc.co.uk/sport/football/${LEAGUE_SLUG}/table"
            FIXTURES_URL="https://www.bbc.co.uk/sport/football/${LEAGUE_SLUG}/fixtures"

            echo "Constructed League URL: $LEAGUE_URL"
            echo "Constructed Fixtures URL: $FIXTURES_URL"

            # --- Set variables for the Python script ---
            echo "TEAM_NAME=${TEAM_SLUG}" >> $GITHUB_ENV
            echo "LEAGUE_URL=${LEAGUE_URL}" >> $GITHUB_ENV
            echo "FIXTURES_URL=${FIXTURES_URL}" >> $GITHUB_ENV
            echo "LEAGUE_ID_NAME=${LEAGUE_NAME}" >> $GITHUB_ENV
            echo "SEASON=${CURRENT_SEASON}" >> $GITHUB_ENV
          else
            echo "❌ ERROR: config.env file not found. Please create it."
            exit 1
          fi

      - name: Create Directories
        run: mkdir -p data tmp public/cal assets/logos

      - name: Run Python Scraper and Data Processor
        id: python_script
        env:
          TEAM_NAME: ${{ env.TEAM_NAME }}
          LEAGUE_URL: ${{ env.LEAGUE_URL }}
          FIXTURES_URL: ${{ env.FIXTURES_URL }}
          LEAGUE_ID_NAME: ${{ env.LEAGUE_ID_NAME }}
          SEASON: ${{ env.SEASON }}
        shell: python
        run: |
          import os
          import json
          import requests
          from bs4 import BeautifulSoup

          # --- Configuration ---
          TEAM_NAME = os.environ.get('TEAM_NAME', '').replace('-', ' ') # Convert slug to name for matching
          LEAGUE_URL = os.environ.get('LEAGUE_URL')
          FIXTURES_URL = os.environ.get('FIXTURES_URL')
          LEAGUE_ID_NAME = os.environ.get('LEAGUE_ID_NAME')
          SEASON = os.environ.get('SEASON')
          HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

          def fetch_html(url, file_path_for_debug=None):
              """Fetches HTML from a URL and optionally saves it."""
              if not url:
                  print("❌ Cannot fetch: URL is missing.")
                  return None
              try:
                  print(f"Fetching {url}...")
                  response = requests.get(url, headers=HEADERS)
                  response.raise_for_status()
                  if file_path_for_debug:
                      with open(file_path_for_debug, 'w', encoding='utf-8') as f:
                          f.write(response.text)
                  print(f"✅ Successfully fetched {url}")
                  return response.text
              except requests.exceptions.RequestException as e:
                  print(f"❌ Error fetching {url}: {e}")
                  return None

          def parse_standings(html_content):
              """Parses the league standings table from BBC Sport HTML."""
              if not html_content: return []
              
              soup = BeautifulSoup(html_content, 'html.parser')
              table = soup.find('table', class_='gs-o-table')

              if not table:
                  print("❌ Could not find the standings table using 'gs-o-table' class.")
                  return []

              standings_data = []
              for row in
