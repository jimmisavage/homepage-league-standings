name: Update Football Data from BBC Sport

on:
  schedule:
    # Runs every 5 minutes past every hour from 12 through 23.
    - cron: '*/5 12-23 * * *'
  workflow_dispatch: # Allows manual running

permissions:
  contents: write

jobs:
  scrape-and-process:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Python Dependencies
        run: pip install beautifulsoup4 requests

      - name: Load Configuration Variables and Build URLs
        id: load_config
        run: |
          if [[ -f "./config.env" ]]; then
            echo "‚úÖ Loading variables from config.env"
            source ./config.env

            # --- Build full URLs from slugs ---
            LEAGUE_URL="https://www.bbc.co.uk/sport/football/${LEAGUE_SLUG}/table"
            FIXTURES_URL="https://www.bbc.co.uk/sport/football/${LEAGUE_SLUG}/scores-fixtures"

            echo "Constructed League URL: $LEAGUE_URL"
            echo "Constructed Fixtures URL: $FIXTURES_URL"

            # --- Set variables for the Python script ---
            echo "TEAM_NAME=${TEAM_SLUG}" >> $GITHUB_ENV
            echo "LEAGUE_URL=${LEAGUE_URL}" >> $GITHUB_ENV
            echo "FIXTURES_URL=${FIXTURES_URL}" >> $GITHUB_ENV
            echo "LEAGUE_ID_NAME=${LEAGUE_NAME}" >> $GITHUB_ENV
            echo "SEASON=${CURRENT_SEASON}" >> $GITHUB_ENV
          else
            echo "‚ùå ERROR: config.env file not found. Please create it."
            exit 1
          fi

      - name: Create Directories
        run: mkdir -p data tmp public/cal assets/logos

      - name: Run Python Scraper and Data Processor
        id: python_script
        env:
          TEAM_NAME: ${{ env.TEAM_NAME }}
          LEAGUE_URL: ${{ env.LEAGUE_URL }}
          FIXTURES_URL: ${{ env.FIXTURES_URL }}
          LEAGUE_ID_NAME: ${{ env.LEAGUE_ID_NAME }}
          SEASON: ${{ env.SEASON }}
        shell: python
        run: |
          import os
          import json
          import requests
          from bs4 import BeautifulSoup

          # --- Configuration ---
          TEAM_NAME = os.environ.get('TEAM_NAME', '').replace('-', ' ') # Convert slug to name for matching
          LEAGUE_URL = os.environ.get('LEAGUE_URL')
          FIXTURES_URL = os.environ.get('FIXTURES_URL')
          LEAGUE_ID_NAME = os.environ.get('LEAGUE_ID_NAME')
          SEASON = os.environ.get('SEASON')
          HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}

          def fetch_html(url, file_path_for_debug=None):
              """Fetches HTML from a URL and optionally saves it."""
              if not url:
                  print("‚ùå Cannot fetch: URL is missing.")
                  return None
              try:
                  print(f"Fetching {url}...")
                  response = requests.get(url, headers=HEADERS)
                  response.raise_for_status()
                  if file_path_for_debug:
                      with open(file_path_for_debug, 'w', encoding='utf-8') as f:
                          f.write(response.text)
                  print(f"‚úÖ Successfully fetched {url}")
                  return response.text
              except requests.exceptions.RequestException as e:
                  print(f"‚ùå Error fetching {url}: {e}")
                  return None

          def parse_standings(html_content):
              """Parses the league standings table from BBC Sport HTML."""
              if not html_content: return []
              
              soup = BeautifulSoup(html_content, 'html.parser')
              table = soup.find('table', class_='gs-o-table')

              if not table:
                  print("‚ùå Could not find the standings table using 'gs-o-table' class.")
                  return []

              standings_data = []
              for row in table.find('tbody').find_all('tr'):
                  cells = row.find_all(['th', 'td'])
                  if len(cells) < 10: continue
                      
                  try:
                      team_name_element = cells[1].find('span', class_=lambda c: c and 'team-name' in c)
                      team_name = team_name_element.get_text(strip=True) if team_name_element else cells[1].get_text(strip=True)

                      team_entry = {
                          'rank': int(cells[0].get_text(strip=True)),
                          'team': {'name': team_name},
                          'points': int(cells[9].get_text(strip=True)),
                          'goalsDiff': int(cells[8].get_text(strip=True)),
                          'all': {
                              'played': int(cells[2].get_text(strip=True)),
                              'win': int(cells[3].get_text(strip=True)),
                              'draw': int(cells[4].get_text(strip=True)),
                              'lose': int(cells[5].get_text(strip=True)),
                          },
                          'highlight': True if TEAM_NAME and TEAM_NAME.lower() in team_name.lower() else False,
                          'nextUp': 'TBD'
                      }
                      standings_data.append(team_entry)
                  except (ValueError, IndexError) as e:
                      print(f"‚ö†Ô∏è Skipping a row in standings due to parsing error: {e}")
                      continue
                      
              print(f"‚úÖ Parsed {len(standings_data)} teams from the standings table.")
              return standings_data
              
          def parse_fixtures(html_content):
              """Parses upcoming fixtures from BBC Sport HTML."""
              if not html_content: return {}
              
              with open('public/cal/fixtures.html', 'w', encoding='utf-8') as f:
                  f.write(html_content)
              
              soup = BeautifulSoup(html_content, 'html.parser')
              fixture_blocks = soup.find_all('div', class_='qa-match-block')
              next_up_map = {}
              
              for block in fixture_blocks:
                  try:
                      home_team_el = block.find('span', class_='sp-c-fixture__team-name--home')
                      away_team_el = block.find('span', class_='sp-c-fixture__team-name--away')
                      
                      if not home_team_el or not away_team_el: continue
                          
                      home_team = home_team_el.find('abbr').get('title')
                      away_team = away_team_el.find('abbr').get('title')
                      
                      if home_team and home_team not in next_up_map:
                          next_up_map[home_team] = f"{away_team} (H)"
                      if away_team and away_team not in next_up_map:
                          next_up_map[away_team] = f"{home_team} (A)"
                  except AttributeError:
                      continue
                          
              print(f"‚úÖ Parsed {len(next_up_map)} upcoming fixture entries.")
              return next_up_map

          def main():
              standings_html = fetch_html(LEAGUE_URL, 'tmp/bbc-standings.html')
              fixtures_html = fetch_html(FIXTURES_URL)
              
              standings = parse_standings(standings_html)
              fixtures_map = parse_fixtures(fixtures_html)
              
              for team in standings:
                  team_name = team['team']['name']
                  if team_name in fixtures_map:
                      team['nextUp'] = fixtures_map[team_name]

              final_json = {
                  'response': [{
                      'league': {
                          'name': LEAGUE_ID_NAME,
                          'season': SEASON,
                          'standings': [standings]
                      }
                  }]
              }

              with open('data/standings.json', 'w', encoding='utf-8') as f:
                  json.dump(final_json, f, indent=4)
                  
              print("üöÄ Successfully created final data/standings.json file.")

          if __name__ == "__main__":
              main()

      - name: Generate League Info File
        run: |
          echo "{\"league_id\": \"$LEAGUE_ID_NAME\", \"season\": \"$SEASON\"}" > data/league-info.json
          echo "‚úÖ Wrote league info to data/league-info.json"

      - name: Commit and Push
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/standings.json data/league-info.json public/cal/fixtures.html
          
          if git diff --staged --quiet; then
            echo "üü¢ No changes detected in data files."
          else
            echo "üíæ Committing updated data..."
            git commit -m "üìä Automated data update from BBC Sport" -m "Date: $(date -u)"
            git push
            echo "‚úÖ Push successful."
          fi
