name: Update Rankings

on:
  schedule:
    - cron: '0 0 * * *' # every day at midnight
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape-and-rank:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v3

      - name: Install Python Dependencies
        run: pip install beautifulsoup4

      - name: Load Configuration Variables
        id: load_config
        run: |
          if [[ -f "./config.env" ]]; then
            echo "Loading variables from config.env"
            source ./config.env
            echo "TEAM_NAME=$TEAM_NAME" >> $GITHUB_ENV
            echo "LEAGUE_URL=$LEAGUE_URL" >> $GITHUB_ENV
            echo "FIXTURES_URL=$FIXTURES_URL" >> $GITHUB_ENV
            echo "SEASON=$SEASON" >> $GITHUB_ENV
            echo "LEAGUE_ID_NAME=$LEAGUE_ID_NAME" >> $GITHUB_ENV
          else
            echo "‚ùå ERROR: config.env file not found. Please create it."
            exit 1
          fi

      - name: Fetch and Parse Standings Table (Python Scraper)
        run: |
          mkdir -p data tmp
          
          # 1. Fetch the HTML content
          echo "Fetching HTML standings from $LEAGUE_URL"
          curl -s "$LEAGUE_URL" -o tmp/bbc-standings.html
          
          # 2. Python script to find the specific table and parse it into JSON
          python3 - <<EOF
import json
from bs4 import BeautifulSoup
import sys
import os

LEAGUE_ID_NAME = os.environ.get('LEAGUE_ID_NAME', 'League')
SEASON = os.environ.get('SEASON', 'Unknown')

try:
    with open('tmp/bbc-standings.html', 'r', encoding='utf-8') as f:
        html_content = f.read()
except FileNotFoundError:
    print("Error: Could not read tmp/bbc-standings.html")
    sys.exit(1)

soup = BeautifulSoup(html_content, 'html.parser')
# --- CRITICAL: Find the table ---
# BBC standings table often has a complex structure. We search for the main table.
standings_table = soup.find('table', class_=lambda c: c and ('table' in c.lower() and 'ssrcss' in c))

if not standings_table:
    print("Error: Could not find the main standings table (check BBC class names).")
    # Write the start of the fetched HTML to a temp file for debugging
    with open('tmp/debug_html_start.txt', 'w') as dbg:
        dbg.write(html_content[:2000]) 
    print("A debug file 'tmp/debug_html_start.txt' was created to inspect the fetched content.")
    sys.exit(1)

data_list = []
# Iterate over all table rows (skipping the header row [0])
tbody = standings_table.find('tbody')
if not tbody:
    print("Error: Table body not found.")
    sys.exit(1)
    
for row in tbody.find_all('tr'):
    cols = row.find_all(['th', 'td'])
    
    # We expect at least 10 columns for a full standings row
    if len(cols) < 10:
        continue 

    try:
        # Extract team name, which is usually inside a span with a specific class
        team_name_element = cols[1].find('span', class_='ssrcss-1c9v1s-Name')
        team_name = team_name_element.get_text(strip=True) if team_name_element else cols[1].get_text(strip=True)
        
        # --- Map data to a simple structure ---
        # Data is extracted using column index (0=Rank, 1=Team, 2=Played, 9=Points)
        team_data = {
            "rank": int(cols[0].get_text(strip=True)),
            "team_name": team_name,
            "played": int(cols[2].get_text(strip=True)),
            "won": int(cols[3].get_text(strip=True)),
            "drawn": int(cols[4].get_text(strip=True)),
            "lost": int(cols[5].get_text(strip=True)),
            "goals_for": int(cols[6].get_text(strip=True)),
            "goals_against": int(cols[7].get_text(strip=True)),
            "goal_difference": int(cols[8].get_text(strip=True)),
            "points": int(cols[9].get_text(strip=True)),
        }
        data_list.append(team_data)
        
    except Exception as e:
        print(f"Skipping row due to parsing error: {e}")
        continue

# Create a simplified JSON structure for the output
final_json = {
    "league_info": {
        "name": LEAGUE_ID_NAME,
        "season": SEASON
    },
    "standings": data_list
}

with open('data/standings.json', 'w') as outfile:
    json.dump(final_json, outfile, indent=4)
    
print("‚úÖ Standings JSON generated successfully: data/standings.json")
EOF

      - name: Fetch Upcoming Fixtures
        run: |
          # Fetch fixtures page HTML for future processing
          mkdir -p public/cal
          echo "Fetching fixtures from $FIXTURES_URL"
          curl -s "$FIXTURES_URL" -o public/cal/fixtures.html
          echo "Future ICS calendar source saved as public/cal/fixtures.html"


      - name: Generate League Info File
        run: |
          # This file is crucial for the second workflow
          echo "{\"league_id\": \"$LEAGUE_ID_NAME\", \"season\": \"$SEASON\"}" > data/league-info.json

      - name: Commit and Push
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@users.noreply.github.com"
          git add data/league-info.json data/standings.json public/cal/fixtures.html
          if git diff --cached --quiet; then
            echo "üü¢ No changes to commit."
          else
            git commit -m "Scrape update: Standings and fixtures from BBC Sport for $LEAGUE_ID_NAME"
            git push
          fi
