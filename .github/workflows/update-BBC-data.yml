name: Update Standings from BBC Sport

on:
  schedule:
    # Runs every 5 minutes on match days (adjust hours/days as needed)
    - cron: '*/5 12-23 * * SAT,SUN'
    # Also runs once late at night to catch up
    - cron: '0 2 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape-standings:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Python Dependencies
        run: pip install beautifulsoup4 requests

      - name: Load Configuration Variables and Build URLs
        id: load_config
        run: |
          if [[ -f "./config.env" ]]; then
            source ./config.env
            LEAGUE_URL="https://www.bbc.co.uk/sport/football/${LEAGUE_SLUG}/table"
            FIXTURES_URL="https://www.bbc.co.uk/sport/football/${LEAGUE_SLUG}/scores-fixtures"
            echo "TEAM_NAME=${TEAM_SLUG}" >> $GITHUB_ENV
            echo "LEAGUE_URL=${LEAGUE_URL}" >> $GITHUB_ENV
            echo "FIXTURES_URL=${FIXTURES_URL}" >> $GITHUB_ENV
            echo "LEAGUE_ID_NAME=${LEAGUE_NAME}" >> $GITHUB_ENV
            echo "SEASON=${CURRENT_SEASON}" >> $GITHUB_ENV
          else
            echo "‚ùå ERROR: config.env file not found."
            exit 1
          fi

      - name: Create Directories
        run: mkdir -p data tmp public/cal

      - name: Run Python Scraper and Data Processor
        id: python_script
        env:
          TEAM_NAME: ${{ env.TEAM_NAME }}
          LEAGUE_URL: ${{ env.LEAGUE_URL }}
          FIXTURES_URL: ${{ env.FIXTURES_URL }}
          LEAGUE_ID_NAME: ${{ env.LEAGUE_ID_NAME }}
          SEASON: ${{ env.SEASON }}
        shell: python
        run: |
          import os
          import json
          import requests
          from bs4 import BeautifulSoup

          TEAM_NAME = os.environ.get('TEAM_NAME', '').replace('-', ' ')
          LEAGUE_URL = os.environ.get('LEAGUE_URL')
          FIXTURES_URL = os.environ.get('FIXTURES_URL')
          LEAGUE_ID_NAME = os.environ.get('LEAGUE_ID_NAME')
          SEASON = os.environ.get('SEASON')
          HEADERS = {'User-Agent': 'Mozilla/5.0'}

          def fetch_html(url):
              if not url: return None
              try:
                  response = requests.get(url, headers=HEADERS)
                  response.raise_for_status()
                  return response.text
              except:
                  return None

          def parse_standings(html_content):
              if not html_content: return []
              soup = BeautifulSoup(html_content, 'html.parser')
              table = soup.find('table', {'data-testid': 'football-table'})
              if not table:
                  print("‚ùå Could not find standings table.")
                  return []
              standings_data = []
              tbody = table.find('tbody')
              if not tbody:
                  print("‚ùå Found table but no tbody.")
                  return []
              for row in tbody.find_all('tr'):
                  cells = row.find_all(['th', 'td'])
                  # Now we expect 11 columns, including form
                  if len(cells) < 11: continue
                  try:
                      rank = int(cells[0].find('span', class_=lambda c: c and 'Rank' in c).get_text(strip=True))
                      team_name_span = cells[0].find('span', class_='visually-hidden')
                      team_name = team_name_span.get_text(strip=True) if team_name_span else "Unknown"
                      logo_img = cells[0].find('img', class_=lambda c: c and 'BadgeImage' in c)
                      logo_url = logo_img['src'] if logo_img else ''
                      
                      played = int(cells[1].get_text(strip=True))
                      won = int(cells[2].get_text(strip=True))
                      drawn = int(cells[3].get_text(strip=True))
                      lost = int(cells[4].get_text(strip=True))
                      goals_for = int(cells[5].get_text(strip=True))
                      goals_against = int(cells[6].get_text(strip=True))
                      goal_diff = int(cells[7].get_text(strip=True))
                      points = int(cells[8].get_text(strip=True))

                      # --- NEW: Extract the form string ---
                      form_cell = cells[9]
                      form_letters = [div.get_text(strip=True) for div in form_cell.find_all('div', {'data-testid': 'letter-content'})]
                      form_string = "".join(form_letters)
                      # ------------------------------------
                      
                      team_entry = {
                          'rank': rank,
                          'team': {'name': team_name, 'logo': logo_url},
                          'points': points, 'goalsDiff': goal_diff, 'form': form_string,
                          'all': {
                              'played': played, 'win': won, 'draw': drawn, 'lose': lost,
                              'goals': { 'for': goals_for, 'against': goals_against }
                          },
                          'highlight': True if TEAM_NAME and TEAM_NAME in team_name.lower() else False,
                          'nextUp': 'TBD'
                      }
                      standings_data.append(team_entry)
                  except Exception as e:
                      print(f"‚ö†Ô∏è Skipping row due to parsing error: {e}")
                      continue
              print(f"‚úÖ Parsed {len(standings_data)} teams.")
              return standings_data

          def parse_fixtures(html_content):
              if not html_content: return {}
              with open('public/cal/fixtures.html', 'w', encoding='utf-8') as f: f.write(html_content)
              soup = BeautifulSoup(html_content, 'html.parser')
              fixture_blocks = soup.find_all('div', class_='qa-match-block')
              next_up_map = {}
              for block in fixture_blocks:
                  try:
                      home_team = block.find('span', class_='sp-c-fixture__team-name--home').find('abbr').get('title')
                      away_team = block.find('span', class_='sp-c-fixture__team-name--away').find('abbr').get('title')
                      if home_team and home_team not in next_up_map: next_up_map[home_team] = f"{away_team} (H)"
                      if away_team and away_team not in next_up_map: next_up_map[away_team] = f"{home_team} (A)"
                  except AttributeError: continue
              print(f"‚úÖ Parsed {len(next_up_map)} upcoming fixture entries.")
              return next_up_map

          def main():
              standings = parse_standings(fetch_html(LEAGUE_URL))
              fixtures_map = parse_fixtures(fetch_html(FIXTURES_URL))
              for team in standings:
                  if team['team']['name'] in fixtures_map: team['nextUp'] = fixtures_map[team['team']['name']]
              final_json = {'response': [{'league': {'name': LEAGUE_ID_NAME, 'season': SEASON, 'standings': [standings]}}]}
              with open('data/standings.json', 'w', encoding='utf-8') as f: json.dump(final_json, f, indent=4)
              print("üöÄ Scrape complete.")

          if __name__ == "__main__": main()

      - name: Commit and Push Standings
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/standings.json
          if [ -f "public/cal/fixtures.html" ]; then
            git add public/cal/fixtures.html
          fi
          if git diff --staged --quiet; then
            echo "üü¢ No changes to standings."
          else
            git commit -m "üìä Automated standings update"
            git push
          fi
