name: Update Standings from BBC Sport

on:
  schedule:
    # Runs every 5 minutes on match days (adjust hours/days as needed)
    - cron: '*/5 12-23 * * SAT,SUN'
    - cron: '*/5 19-23 * * MON-FRI'
    # Also runs once late at night to catch up
    - cron: '0 2 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape-standings:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Python Dependencies
        run: pip install beautifulsoup4 requests python-dateutil

      - name: Load Configuration Variables
        id: load_config
        run: |
          if [[ -f "./config.env" ]]; then
            source ./config.env
            echo "LEAGUE_SLUG=${LEAGUE_SLUG}" >> $GITHUB_ENV
            echo "TEAM_NAME=${TEAM_SLUG}" >> $GITHUB_ENV
            echo "LEAGUE_ID_NAME=${LEAGUE_NAME}" >> $GITHUB_ENV
            echo "SEASON=${CURRENT_SEASON}" >> $GITHUB_ENV
          else
            echo "‚ùå ERROR: config.env file not found."
            exit 1
          fi

      - name: Run Python Scraper and Data Processor
        id: python_script
        env:
          LEAGUE_SLUG: ${{ env.LEAGUE_SLUG }}
          TEAM_NAME: ${{ env.TEAM_NAME }}
          LEAGUE_ID_NAME: ${{ env.LEAGUE_ID_NAME }}
          SEASON: ${{ env.SEASON }}
        shell: python
        run: |
          import os
          import json
          import requests
          from bs4 import BeautifulSoup
          from datetime import datetime
          from dateutil.relativedelta import relativedelta

          TEAM_NAME = os.environ.get('TEAM_NAME', '').replace('-', ' ')
          LEAGUE_SLUG = os.environ.get('LEAGUE_SLUG')
          LEAGUE_ID_NAME = os.environ.get('LEAGUE_ID_NAME')
          SEASON = os.environ.get('SEASON')
          HEADERS = {'User-Agent': 'Mozilla/5.0'}

          def fetch_html(url):
              if not url: return None
              try:
                  print(f"Fetching {url}...")
                  response = requests.get(url, headers=HEADERS)
                  response.raise_for_status()
                  return response.text
              except Exception as e:
                  print(f"‚ùå Error fetching {url}: {e}")
                  return None

          def parse_standings(html_content):
              # This function is correct and does not need to change
              if not html_content: return []
              soup = BeautifulSoup(html_content, 'html.parser')
              table = soup.find('table', {'data-testid': 'football-table'})
              if not table:
                  print("‚ùå Could not find standings table.")
                  return []
              standings_data = []
              tbody = table.find('tbody')
              if not tbody: return []
              for row in tbody.find_all('tr'):
                  cells = row.find_all(['th', 'td'])
                  if len(cells) < 10: continue
                  try:
                      rank = int(cells[0].find('span', class_=lambda c: c and 'Rank' in c).get_text(strip=True))
                      team_name_span = cells[0].find('span', class_='visually-hidden')
                      team_name = team_name_span.get_text(strip=True) if team_name_span else "Unknown"
                      logo_img = cells[0].find('img', class_=lambda c: c and 'BadgeImage' in c)
                      logo_url = logo_img['src'] if logo_img else ''
                      played, won, drawn, lost = map(int, [c.get_text(strip=True) for c in cells[1:5]])
                      goals_for, goals_against, goal_diff, points = map(int, [c.get_text(strip=True) for c in cells[5:9]])
                      form_cell = cells[9]
                      form_letters = [div.get_text(strip=True) for div in form_cell.find_all('div', {'data-testid': 'letter-content'})]
                      form_string = "".join(form_letters)
                      team_entry = {
                          'rank': rank, 'team': {'name': team_name, 'logo': logo_url}, 'points': points,
                          'goalsDiff': goal_diff, 'form': form_string,
                          'all': {
                              'played': played, 'win': won, 'draw': drawn, 'lose': lost,
                              'goals': { 'for': goals_for, 'against': goals_against }
                          },
                          'highlight': True if TEAM_NAME and TEAM_NAME in team_name.lower() else False, 'nextUp': 'TBD'
                      }
                      standings_data.append(team_entry)
                  except: continue
              print(f"‚úÖ Parsed {len(standings_data)} teams from BBC table.")
              return standings_data

          def parse_fixtures(html_content):
              # This function is correct and does not need to change
              if not html_content: return {}
              soup = BeautifulSoup(html_content, 'html.parser')
              next_up_map = {}
              all_fixtures = soup.find_all('li', attrs={'data-tipo-topic-id': True})
              for fixture in all_fixtures:
                  try:
                      home_team_div = fixture.select_one('div[class*="TeamHome"]')
                      away_team_div = fixture.select_one('div[class*="TeamAway"]')
                      if home_team_div and away_team_div:
                          home_team_span = home_team_div.find('span', class_='visually-hidden')
                          away_team_span = away_team_div.find('span', class_='visually-hidden')
                          if home_team_span and away_team_span:
                              home_team = home_team_span.get_text(strip=True)
                              away_team = away_team_span.get_text(strip=True)
                              if home_team and home_team not in next_up_map:
                                  next_up_map[home_team] = f"{away_team} (H)"
                              if away_team and away_team not in next_up_map:
                                  next_up_map[away_team] = f"{home_team} (A)"
                  except: continue
              return next_up_map

          def main():
              # --- THIS FUNCTION IS NOW SMARTER ---
              base_bbc_url = f"https://www.bbc.co.uk/sport/football/{LEAGUE_SLUG}"
              standings_url = f"{base_bbc_url}/table"

              # Generate URLs for this month and next month's fixtures
              today = datetime.now()
              next_month_date = today + relativedelta(months=1)
              url_this_month = f"{base_bbc_url}/scores-fixtures/{today.strftime('%Y-%m')}"
              url_next_month = f"{base_bbc_url}/scores-fixtures/{next_month_date.strftime('%Y-%m')}"

              # Fetch all data
              standings_html = fetch_html(standings_url)
              fixtures_html_this_month = fetch_html(url_this_month)
              fixtures_html_next_month = fetch_html(url_next_month)
              
              # Parse standings
              standings = parse_standings(standings_html)
              
              # Parse fixtures from both months and combine them
              fixtures_map = parse_fixtures(fixtures_html_this_month)
              # .update() merges the second dictionary into the first
              fixtures_map.update(parse_fixtures(fixtures_html_next_month))
              print(f"‚úÖ Combined fixtures from two months, total: {len(fixtures_map)} entries.")

              # Map fixtures to standings
              for team in standings:
                  if team['team']['name'] in fixtures_map:
                      team['nextUp'] = fixtures_map[team['team']['name']]
              
              final_json = {'response': [{'league': {'name': LEAGUE_ID_NAME, 'season': SEASON, 'standings': [standings]}}]}
              with open('data/standings.json', 'w', encoding='utf-8') as f:
                  json.dump(final_json, f, indent=4)
              print("üöÄ Scrape complete.")

          if __name__ == "__main__": main()

      - name: Commit and Push Standings
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/standings.json
          if git diff --staged --quiet; then
            echo "üü¢ No changes to standings."
          else
            git commit -m "üìä Automated standings update"
            git push
          fi
