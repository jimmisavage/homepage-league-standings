name: Update Standings from BBC Sport

on:
  schedule:
    # Runs every 5 minutes on weekend match days (12:00-23:00)
    - cron: '*/5 12-23 * * SAT,SUN'
    # Runs every 5 minutes on weeknight match days (19:00-23:00)
    - cron: '*/5 19-23 * * MON-FRI'
    # Also runs once late at night to catch up
    - cron: '0 2 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape-standings:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Python Dependencies
        run: pip install beautifulsoup4 requests

      - name: Load Configuration Variables
        id: load_config
        run: |
          if [[ -f "./config.env" ]]; then
            source ./config.env
            # Build BBC Table URL
            LEAGUE_URL="https://www.bbc.co.uk/sport/football/${LEAGUE_SLUG}/table"
            
            # Pass all variables to the environment
            echo "TEAM_NAME=${TEAM_SLUG}" >> $GITHUB_ENV
            echo "LEAGUE_URL=${LEAGUE_URL}" >> $GITHUB_ENV
            echo "EFL_FIXTURES_URL=${EFL_FIXTURES_URL}" >> $GITHUB_ENV # Read new EFL URL
            echo "LEAGUE_ID_NAME=${LEAGUE_NAME}" >> $GITHUB_ENV
            echo "SEASON=${CURRENT_SEASON}" >> $GITHUB_ENV
          else
            echo "‚ùå ERROR: config.env file not found."
            exit 1
          fi

      - name: Create Directories
        run: mkdir -p data

      - name: Run Python Scraper and Data Processor
        id: python_script
        env:
          TEAM_NAME: ${{ env.TEAM_NAME }}
          LEAGUE_URL: ${{ env.LEAGUE_URL }}
          EFL_FIXTURES_URL: ${{ env.EFL_FIXTURES_URL }}
          LEAGUE_ID_NAME: ${{ env.LEAGUE_ID_NAME }}
          SEASON: ${{ env.SEASON }}
        shell: python
        run: |
          import os
          import json
          import requests
          from bs4 import BeautifulSoup

          TEAM_NAME = os.environ.get('TEAM_NAME', '').replace('-', ' ')
          LEAGUE_URL = os.environ.get('LEAGUE_URL')
          FIXTURES_URL = os.environ.get('EFL_FIXTURES_URL') # Use the new EFL URL
          LEAGUE_ID_NAME = os.environ.get('LEAGUE_ID_NAME')
          SEASON = os.environ.get('SEASON')
          HEADERS = {'User-Agent': 'Mozilla/5.0'}

          def fetch_html(url):
              if not url: return None
              try:
                  print(f"Fetching {url}...")
                  response = requests.get(url, headers=HEADERS)
                  response.raise_for_status()
                  return response.text
              except Exception as e:
                  print(f"‚ùå Error fetching {url}: {e}")
                  return None

          def parse_standings(html_content):
              # This function remains the same as it correctly parses the BBC table
              if not html_content: return []
              soup = BeautifulSoup(html_content, 'html.parser')
              table = soup.find('table', {'data-testid': 'football-table'})
              if not table:
                  print("‚ùå Could not find standings table.")
                  return []
              standings_data = []
              tbody = table.find('tbody')
              if not tbody:
                  print("‚ùå Found table but no tbody.")
                  return []
              for row in tbody.find_all('tr'):
                  cells = row.find_all(['th', 'td'])
                  if len(cells) < 10: continue
                  try:
                      rank = int(cells[0].find('span', class_=lambda c: c and 'Rank' in c).get_text(strip=True))
                      team_name_span = cells[0].find('span', class_='visually-hidden')
                      team_name = team_name_span.get_text(strip=True) if team_name_span else "Unknown"
                      logo_img = cells[0].find('img', class_=lambda c: c and 'BadgeImage' in c)
                      logo_url = logo_img['src'] if logo_img else ''
                      played, won, drawn, lost = map(int, [c.get_text(strip=True) for c in cells[1:5]])
                      goals_for, goals_against, goal_diff, points = map(int, [c.get_text(strip=True) for c in cells[5:9]])
                      form_cell = cells[9]
                      form_letters = [div.get_text(strip=True) for div in form_cell.find_all('div', {'data-testid': 'letter-content'})]
                      form_string = "".join(form_letters)
                      team_entry = {
                          'rank': rank, 'team': {'name': team_name, 'logo': logo_url}, 'points': points,
                          'goalsDiff': goal_diff, 'form': form_string,
                          'all': {
                              'played': played, 'win': won, 'draw': drawn, 'lose': lost,
                              'goals': { 'for': goals_for, 'against': goals_against }
                          },
                          'highlight': True if TEAM_NAME and TEAM_NAME in team_name.lower() else False, 'nextUp': 'TBD'
                      }
                      standings_data.append(team_entry)
                  except Exception as e:
                      print(f"‚ö†Ô∏è Skipping row due to parsing error: {e}")
                      continue
              print(f"‚úÖ Parsed {len(standings_data)} teams from BBC table.")
              return standings_data

          def parse_fixtures(html_content):
              # --- THIS FUNCTION IS COMPLETELY REWRITTEN FOR EFL.COM ---
              if not html_content: return {}
              soup = BeautifulSoup(html_content, 'html.parser')
              next_up_map = {}
              
              # Find all fixture blocks using their specific class
              all_fixtures = soup.find_all('div', class_='match-item')

              for fixture in all_fixtures:
                  try:
                      # Fixtures that are already played have a score, future ones don't
                      score = fixture.find('div', class_='match-item__score')
                      if score:
                          continue # Skip this match, it's already happened

                      home_team_name = fixture.find('div', class_='match-item__team--home').find('p').get_text(strip=True)
                      away_team_name = fixture.find('div', class_='match-item__team--away').find('p').get_text(strip=True)

                      if home_team_name and home_team_name not in next_up_map:
                          next_up_map[home_team_name] = f"{away_team_name} (H)"
                      if away_team_name and away_team_name not in next_up_map:
                          next_up_map[away_team_name] = f"{away_team_name} (A)"
                  except AttributeError:
                      continue
                          
              print(f"‚úÖ Parsed {len(next_up_map)} upcoming fixture entries from EFL.com.")
              return next_up_map

          def main():
              standings = parse_standings(fetch_html(LEAGUE_URL))
              fixtures_map = parse_fixtures(fetch_html(FIXTURES_URL)) # Now fetches from EFL.com
              
              for team in standings:
                  # Team names from BBC (e.g. "Huddersfield Town") must match EFL.com names
                  if team['team']['name'] in fixtures_map:
                      team['nextUp'] = fixtures_map[team['team']['name']]
              
              final_json = {'response': [{'league': {'name': LEAGUE_ID_NAME, 'season': SEASON, 'standings': [standings]}}]}
              with open('data/standings.json', 'w', encoding='utf-8') as f:
                  json.dump(final_json, f, indent=4)
              print("üöÄ Scrape complete.")

          if __name__ == "__main__": main()

      - name: Commit and Push Standings
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/standings.json
          if git diff --staged --quiet; then
            echo "üü¢ No changes to standings."
          else
            git commit -m "üìä Automated standings update"
            git push
          fi
