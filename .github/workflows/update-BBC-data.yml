name: Update Standings from BBC Sport

on:
  schedule:
    - cron: '*/5 12-23 * * SAT,SUN'
    - cron: '*/5 19-23 * * MON-FRI'
    - cron: '0 2 * * *'
  workflow_dispatch:

permissions:
  contents: write
  actions: read

jobs:
  scrape-standings:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Python Dependencies
        run: pip install beautifulsoup4 requests

      - name: Load Configuration Variables
        id: load_config
        run: |
          if [[ -f "./config.env" ]]; then
            source ./config.env
            LEAGUE_URL="https://www.bbc.co.uk/sport/football/${LEAGUE_SLUG}/table"
            echo "TEAM_NAME=${TEAM_SLUG}" >> $GITHUB_ENV
            echo "LEAGUE_URL=${LEAGUE_URL}" >> $GITHUB_ENV
            echo "EFL_FIXTURES_URL=${EFL_FIXTURES_URL}" >> $GITHUB_ENV
            echo "LEAGUE_ID_NAME=${LEAGUE_NAME}" >> $GITHUB_ENV
            echo "SEASON=${CURRENT_SEASON}" >> $GITHUB_ENV
          else
            echo "‚ùå ERROR: config.env file not found."
            exit 1
          fi

      - name: Create Directories
        run: mkdir -p data tmp

      - name: Run Python Scraper and Data Processor
        id: python_script
        env:
          TEAM_NAME: ${{ env.TEAM_NAME }}
          LEAGUE_URL: ${{ env.LEAGUE_URL }}
          EFL_FIXTURES_URL: ${{ env.EFL_FIXTURES_URL }}
          LEAGUE_ID_NAME: ${{ env.LEAGUE_ID_NAME }}
          SEASON: ${{ env.SEASON }}
        shell: python
        run: |
          import os
          import json
          import requests
          from bs4 import BeautifulSoup

          TEAM_NAME = os.environ.get('TEAM_NAME', '').replace('-', ' ')
          LEAGUE_URL = os.environ.get('LEAGUE_URL')
          FIXTURES_URL = os.environ.get('EFL_FIXTURES_URL')
          LEAGUE_ID_NAME = os.environ.get('LEAGUE_ID_NAME')
          SEASON = os.environ.get('SEASON')
          HEADERS = {'User-Agent': 'Mozilla/5.0'}

          def fetch_html(url, file_path_for_debug=None):
              if not url: return None
              try:
                  print(f"Fetching {url}...")
                  response = requests.get(url, headers=HEADERS)
                  response.raise_for_status()
                  if file_path_for_debug:
                      with open(file_path_for_debug, 'w', encoding='utf-8') as f: f.write(response.text)
                  return response.text
              except Exception as e:
                  print(f"‚ùå Error fetching {url}: {e}")
                  return None

          def parse_standings(html_content):
              if not html_content: return []
              soup = BeautifulSoup(html_content, 'html.parser')
              table = soup.find('table', {'data-testid': 'football-table'})
              if not table: return []
              standings_data = []
              tbody = table.find('tbody')
              if not tbody: return []
              for row in tbody.find_all('tr'):
                  cells = row.find_all(['th', 'td'])
                  if len(cells) < 10: continue
                  try:
                      rank = int(cells[0].find('span', class_=lambda c: c and 'Rank' in c).get_text(strip=True))
                      team_name_span = cells[0].find('span', class_='visually-hidden')
                      team_name = team_name_span.get_text(strip=True) if team_name_span else "Unknown"
                      logo_img = cells[0].find('img', class_=lambda c: c and 'BadgeImage' in c)
                      logo_url = logo_img['src'] if logo_img else ''
                      played, won, drawn, lost = map(int, [c.get_text(strip=True) for c in cells[1:5]])
                      goals_for, goals_against, goal_diff, points = map(int, [c.get_text(strip=True) for c in cells[5:9]])
                      form_cell = cells[9]
                      form_letters = [div.get_text(strip=True) for div in form_cell.find_all('div', {'data-testid': 'letter-content'})]
                      form_string = "".join(form_letters)
                      team_entry = {
                          'rank': rank, 'team': {'name': team_name, 'logo': logo_url}, 'points': points,
                          'goalsDiff': goal_diff, 'form': form_string,
                          'all': {
                              'played': played, 'win': won, 'draw': drawn, 'lose': lost,
                              'goals': { 'for': goals_for, 'against': goals_against }
                          },
                          'highlight': True if TEAM_NAME and TEAM_NAME in team_name.lower() else False, 'nextUp': 'TBD'
                      }
                      standings_data.append(team_entry)
                  except: continue
              print(f"‚úÖ Parsed {len(standings_data)} teams from BBC table.")
              return standings_data

          def parse_fixtures(html_content):
              if not html_content: return {}
              soup = BeautifulSoup(html_content, 'html.parser')
              next_up_map = {}
              all_fixtures = soup.find_all('div', class_='match-item')
              for fixture in all_fixtures:
                  try:
                      score = fixture.find('div', class_='match-item__score')
                      if score: continue
                      home_team_name = fixture.find('div', class_='match-item__team--home').find('p').get_text(strip=True)
                      away_team_name = fixture.find('div', class_='match-item__team--away').find('p').get_text(strip=True)
                      if home_team_name and home_team_name not in next_up_map:
                          next_up_map[home_team_name] = f"{away_team_name} (H)"
                      if away_team_name and away_team_name not in next_up_map:
                          next_up_map[away_team_name] = f"{away_team_name} (A)"
                  except: continue
              print(f"‚úÖ Parsed {len(next_up_map)} upcoming fixture entries from EFL.com.")
              return next_up_map

          def main():
              standings_html = fetch_html(LEAGUE_URL)
              fixtures_html = fetch_html(FIXTURES_URL, 'tmp/efl-fixtures.html') # Save EFL html for debugging
              standings = parse_standings(standings_html)
              fixtures_map = parse_fixtures(fixtures_html)
              for team in standings:
                  if team['team']['name'] in fixtures_map:
                      team['nextUp'] = fixtures_map[team['team']['name']]
              final_json = {'response': [{'league': {'name': LEAGUE_ID_NAME, 'season': SEASON, 'standings': [standings]}}]}
              with open('data/standings.json', 'w', encoding='utf-8') as f: json.dump(final_json, f, indent=4)
              print("üöÄ Scrape complete.")

          if __name__ == "__main__": main()

      - name: üíæ Upload EFL Debug Artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: efl-fixtures-html-output
          path: tmp/efl-fixtures.html

      - name: Commit and Push Standings
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/standings.json
          if git diff --staged --quiet; then
            echo "üü¢ No changes to standings."
          else
            git commit -m "üìä Automated standings update"
            git push
          fi
