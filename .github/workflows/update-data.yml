name: Update data

on:
  schedule:
    - cron: '*/5 12-23 * * SAT,SUN'
    - cron: '*/5 19-23 * * MON-FRI'
    - cron: '0 2 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape-and-generate:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Python Dependencies
        run: pip install beautifulsoup4 requests

      - name: Load Configuration Variables
        id: load_config
        run: |
          if [[ -f "./config.env" ]]; then
            source ./config.env
            echo "LEAGUE_SLUG=${LEAGUE_SLUG}" >> $GITHUB_ENV
            echo "TEAM_NAME=${TEAM_SLUG}" >> $GITHUB_ENV
            echo "LEAGUE_ID_NAME=${LEAGUE_NAME}" >> $GITHUB_ENV
            echo "SEASON=${CURRENT_SEASON}" >> $GITHUB_ENV
          else
            echo "‚ùå ERROR: config.env file not found."
            exit 1
          fi

      - name: Run Python Scraper
        id: python_scraper
        env:
          LEAGUE_SLUG: ${{ env.LEAGUE_SLUG }}
          TEAM_NAME: ${{ env.TEAM_NAME }}
          LEAGUE_ID_NAME: ${{ env.LEAGUE_ID_NAME }}
          SEASON: ${{ env.SEASON }}
        shell: python
        run: |
          import os
          import json
          import requests
          from bs4 import BeautifulSoup
          from datetime import datetime, timedelta

          TEAM_NAME = os.environ.get('TEAM_NAME', '').replace('-', ' ')
          LEAGUE_SLUG = os.environ.get('LEAGUE_SLUG')
          LEAGUE_ID_NAME = os.environ.get('LEAGUE_ID_NAME')
          SEASON = os.environ.get('SEASON')
          HEADERS = {'User-Agent': 'Mozilla/5.0'}

          def fetch_html(url):
              if not url: return None
              try:
                  print(f"Fetching {url}...")
                  response = requests.get(url, headers=HEADERS)
                  response.raise_for_status()
                  return response.text
              except Exception as e:
                  print(f"‚ùå Error fetching {url}: {e}")
                  return None

          def parse_standings(html_content):
              if not html_content: return []
              soup = BeautifulSoup(html_content, 'html.parser')
              table = soup.find('table', {'data-testid': 'football-table'})
              if not table: return []
              standings_data = []
              tbody = table.find('tbody')
              if not tbody: return []
              for row in tbody.find_all('tr'):
                  cells = row.find_all(['th', 'td'])
                  if len(cells) < 10: continue
                  try:
                      team_name_span = cells[0].find('span', class_='visually-hidden')
                      team_name = team_name_span.get_text(strip=True) if team_name_span else "Unknown"
                      standings_data.append({
                          'rank': int(cells[0].find('span', class_=lambda c: c and 'Rank' in c).get_text(strip=True)),
                          'team': {'name': team_name, 'logo': cells[0].find('img')['src'] if cells[0].find('img') else ''},
                          'points': int(cells[8].get_text(strip=True)),
                          'goalsDiff': int(cells[7].get_text(strip=True)),
                          'form': "".join([div.get_text(strip=True) for div in cells[9].find_all('div', {'data-testid': 'letter-content'})]),
                          'all': {
                              'played': int(cells[1].get_text(strip=True)), 'win': int(cells[2].get_text(strip=True)),
                              'draw': int(cells[3].get_text(strip=True)), 'lose': int(cells[4].get_text(strip=True)),
                              'goals': {'for': int(cells[5].get_text(strip=True)), 'against': int(cells[6].get_text(strip=True))}
                          },
                          'highlight': True if TEAM_NAME and TEAM_NAME in team_name.lower() else False, 'nextUp': 'TBD'
                      })
                  except: continue
              print(f"‚úÖ Parsed {len(standings_data)} teams from BBC table.")
              return standings_data

          def parse_fixtures(html_content):
              if not html_content: return []
              soup = BeautifulSoup(html_content, 'html.parser')
              fixtures = []
              try:
                  print("--- Fixture Debug Start ---")
                  print("1. Searching for INITIAL_DATA script tag...")
                  script_tag = soup.find('script', string=lambda t: t and 'window.__INITIAL_DATA__' in t)
                  if not script_tag:
                      print("‚ùå Step 1 FAILED: Script tag not found.")
                      return []
                  print("‚úÖ Step 1 SUCCESS: Found script tag.")

                  print("2. Extracting JSON text...")
                  json_text = script_tag.string.split('window.__INITIAL_DATA__=')[1].split(';')[0]
                  if not json_text:
                      print("‚ùå Step 2 FAILED: Could not extract JSON text.")
                      return []
                  print("‚úÖ Step 2 SUCCESS: Extracted JSON text.")

                  print("3. Parsing JSON data...")
                  data = json.loads(json_text)
                  print("‚úÖ Step 3 SUCCESS: Parsed JSON.")

                  print("4. Searching for 'sport-data-scores-fixtures' key in data...")
                  found_fixtures_key = False
                  for key, value in data.get('data', {}).items():
                      if 'sport-data-scores-fixtures' in key:
                          found_fixtures_key = True
                          print(f"‚úÖ Step 4 SUCCESS: Found key '{key}'.")
                          print("5. Navigating eventGroups...")
                          event_groups = value.get('data', {}).get('eventGroups', [])
                          if not event_groups:
                              print("‚ö†Ô∏è Step 5 WARNING: 'eventGroups' list is empty.")
                          
                          for group in event_groups:
                              for secondary_group in group.get('secondaryGroups', []):
                                  for event in secondary_group.get('events', []):
                                      if event.get('status') == 'PreEvent':
                                          fixtures.append({
                                              'home_team': event['home']['fullName'],
                                              'away_team': event['away']['fullName'],
                                              'datetime_utc': event['startDateTime']
                                          })
                          break
                  if not found_fixtures_key:
                      print("‚ùå Step 4 FAILED: Could not find 'sport-data-scores-fixtures' key.")
              except Exception as e:
                  print(f"‚ùå An unexpected error occurred: {e}")

              print(f"--- Fixture Debug End ---")
              print(f"‚úÖ Parsed {len(fixtures)} fixture details from embedded JSON.")
              return fixtures

          def main():
              base_bbc_url = f"https://www.bbc.co.uk/sport/football/{LEAGUE_SLUG}"
              standings_url = f"{base_bbc_url}/table"
              fixtures_url = f"{base_bbc_url}/scores-fixtures"
              
              standings_html = fetch_html(standings_url)
              fixtures_html = fetch_html(fixtures_url)
              
              standings = parse_standings(standings_html)
              all_fixtures = parse_fixtures(fixtures_html)

              next_up_map = {}
              for fix in sorted(all_fixtures, key=lambda x: x['datetime_utc']):
                  if fix['home_team'] not in next_up_map:
                      next_up_map[fix['home_team']] = f"{fix['away_team']} (H)"
                  if fix['away_team'] not in next_up_map:
                      next_up_map[fix['away_team']] = f"{fix['home_team']} (A)"

              for team in standings:
                  if team['team']['name'] in next_up_map:
                      team['nextUp'] = next_up_map[team['team']['name']]
              
              os.makedirs('data', exist_ok=True)
              with open('data/standings.json', 'w') as f:
                  json.dump({'response': [{'league': {'name': LEAGUE_ID_NAME, 'season': SEASON, 'standings': [standings]}}]}, f, indent=4)
              with open('data/fixtures.json', 'w') as f:
                  json.dump(all_fixtures, f, indent=4)
              print("üöÄ Scrape complete.")

          if __name__ == "__main__": main()
      
      - name: Generate EPG File
        shell: python
        env:
          LEAGUE_ID_NAME: ${{ env.LEAGUE_ID_NAME }}
        run: |
          import json
          import os
          from datetime import datetime, timedelta

          LEAGUE_ID_NAME = os.environ.get('LEAGUE_ID_NAME')

          def create_epg():
              try:
                  with open('data/standings.json', 'r') as f:
                      standings_data = json.load(f)['response'][0]['league']['standings'][0]
                  with open('data/fixtures.json', 'r') as f:
                      fixtures_data = json.load(f)
              except (IOError, KeyError, IndexError):
                  print("‚ùå Standings or fixtures data not found. Cannot generate EPG.")
                  return

              xml = '<?xml version="1.0" encoding="UTF-8"?>\n<tv>'
              
              for team in standings_data:
                  team_name = team['team']['name']
                  team_slug = team_name.lower().replace('&', 'and').replace(' ', '-')
                  team_slug = ''.join(c for c in team_slug if c.isalnum() or c == '-')
                  xml += f'\n  <channel id="{team_slug}.uk"><display-name lang="en">{team_name}</display-name></channel>'

              for fixture in fixtures_data:
                  home_team = fixture['home_team']
                  away_team = fixture['away_team']
                  home_slug = ''.join(c for c in home_team.lower().replace('&', 'and').replace(' ', '-') if c.isalnum() or c == '-')
                  away_slug = ''.join(c for c in away_team.lower().replace('&', 'and').replace(' ', '-') if c.isalnum() or c == '-')
                  
                  dt_start = datetime.fromisoformat(fixture['datetime_utc'].replace('Z', '+00:00'))
                  dt_stop = dt_start + timedelta(hours=2)
                  start_str = dt_start.strftime('%Y%m%d%H%M%S %z')
                  stop_str = dt_stop.strftime('%Y%m%d%H%M%S %z')

                  xml += f'\n  <programme start="{start_str}" stop="{stop_str}" channel="{home_slug}.uk">'
                  xml += f'<title lang="en">vs. {away_team} (H)</title><desc lang="en">{LEAGUE_ID_NAME}</desc><category lang="en">Sports</category></programme>'
                  
                  xml += f'\n  <programme start="{start_str}" stop="{stop_str}" channel="{away_slug}.uk">'
                  xml += f'<title lang="en">vs. {home_team} (A)</title><desc lang="en">{LEAGUE_ID_NAME}</desc><category lang="en">Sports</category></programme>'

              xml += '\n</tv>'

              with open('epg.xml', 'w', encoding='utf-8') as f:
                  f.write(xml)
              print("‚úÖ EPG file generated successfully.")

          create_epg()

      - name: Commit and Push Data
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/standings.json data/fixtures.json epg.xml
          if git diff --staged --quiet; then
            echo "üü¢ No data changes to commit."
          else
            git commit -m "üìä Automated standings and EPG update"
            git push
          fi
